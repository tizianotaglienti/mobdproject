{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "file_name = \"pickles.zip\"\n",
    "\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "    zip.extractall()\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = joblib.load(\"pickles/imputer.pkl\")\n",
    "scaler = joblib.load(\"pickles/scaler.pkl\")\n",
    "bestclf = joblib.load(\"pickles/model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test_set.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asia = [\"Philippines\", \"India\", \"South\", \"China\", \"Vietnam\", \"Japan\", \"Taiwan\", \"Iran\", \"Thailand\", \"Cambodia\", \"Laos\", \"Hong\"]\n",
    "europe = [\"Germany\", \"England\", \"Italy\", \"Poland\", \"Portugal\", \"Greece\", \"France\", \"Ireland\", \"Yugoslavia\", \"Hungary\", \"Scotland\", \"Netherlands\"]\n",
    "north_america = [\"USA\", \"Canada\"]\n",
    "central_america = [\"Mexico\", \"Puerto-Rico\", \"El-Salvador\", \"Cuba\", \"Jamaica\", \"Dominican-Republic\", \"Guatemala\", \"Haiti\", \"Nicaragua\", \"Caribbean\", \"Honduras\"]\n",
    "south_america = [\"Columbia\", \"Peru\", \"Ecuador\"]\n",
    "\n",
    "for i in range(len(test)):\n",
    "    if test.loc[i, \"F13\"] in asia:\n",
    "        test.F13.replace(test.loc[i, \"F13\"], \"asia\", inplace = True)\n",
    "    elif test.loc[i, \"F13\"] in europe:\n",
    "        test.F13.replace(test.loc[i, \"F13\"], \"europe\", inplace = True)\n",
    "    elif test.loc[i, \"F13\"] in north_america:\n",
    "        test.F13.replace(test.loc[i, \"F13\"], \"north_america\", inplace = True)\n",
    "    elif test.loc[i, \"F13\"] in central_america:\n",
    "        test.F13.replace(test.loc[i, \"F13\"], \"central_america\", inplace = True)\n",
    "    elif test.loc[i, \"F13\"] in south_america:\n",
    "        test.F13.replace(test.loc[i, \"F13\"], \"south_america\", inplace = True)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colF1 = ['K1', 'K2', 'K3', 'K4', 'K5', 'K6', 'K7', 'K8']\n",
    "colF3 = ['R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'R9', 'R10', 'R11', 'R12', 'R13', 'R14', 'R15', 'R16']\n",
    "colF5 = ['Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7']\n",
    "colF6 = ['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 'P12', 'P13', 'P14']\n",
    "colF7 = ['M1', 'M2', 'M3', 'M4', 'M5', 'M6']\n",
    "colF8 = ['caucasian', 'black', 'asian', 'american', 'other']\n",
    "colF9 = ['Male', 'Female']\n",
    "colF13 = ['asia', 'europe', 'north_america', 'central_america', 'south_america']\n",
    "\n",
    "test.F1.replace(colF1, [0, 1, 2, 3, 4, 5, 6, 7], inplace = True)\n",
    "test.F3.replace(colF3, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], inplace = True)\n",
    "test.F5.replace(colF5, [0, 1, 2, 3, 4, 5, 6], inplace = True)\n",
    "test.F6.replace(colF6, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], inplace = True)\n",
    "test.F7.replace(colF7, [0, 1, 2, 3, 4, 5], inplace = True)\n",
    "test.F8.replace(colF8, [0, 1, 2, 3, 4], inplace = True)\n",
    "test.F9.replace(colF9, [0, 1], inplace = True)\n",
    "test.F13.replace(colF13, [0, 1, 2, 3, 4], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test.iloc[:, :-1].values\n",
    "y_test = test.iloc[:, -1].values\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputazione dei valori NaN tramite SimpleImputer, con la strategia che sostituisce i valori mancanti con la moda dei valori della feature\n",
    "# print(\"SimpleImputer\")\n",
    "# imp = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\n",
    "# x_test_notnan = imp.transform(x_test)\n",
    "# print(x_test_notnan[pd.isna(x_test[:,0]),0][0:10])\n",
    "\n",
    "x_test[:] = imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione DataFrame da insiemi di train e test dopo l'imputazione dei valori mancanti \n",
    "colF = ['F0','F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13']\n",
    "\n",
    "test_set_df = pd.DataFrame(x_test_notnan, columns = colF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codifica OneHot contemporaneamente su training_set_df e test_set_df\n",
    "OneHotTestF1 = pd.get_dummies(test_set_df.F1)\n",
    "OneHotTestF3 = pd.get_dummies(test_set_df.F3)\n",
    "OneHotTestF5 = pd.get_dummies(test_set_df.F5)\n",
    "OneHotTestF6 = pd.get_dummies(test_set_df.F6)\n",
    "OneHotTestF7 = pd.get_dummies(test_set_df.F7)\n",
    "OneHotTestF8 = pd.get_dummies(test_set_df.F8)\n",
    "OneHotTestF9 = pd.get_dummies(test_set_df.F9)\n",
    "OneHotTestF13 = pd.get_dummies(test_set_df.F13)\n",
    "\n",
    "for i in range (0, len(colF1)):\n",
    "    OneHotTestF1 = OneHotTestF1.rename(columns = {i: colF1[i]})\n",
    "for i in range (0, len(colF3)):\n",
    "    OneHotTestF3 = OneHotTestF3.rename(columns = {i: colF3[i]})\n",
    "for i in range (0, len(colF5)):\n",
    "    OneHotTestF5 = OneHotTestF5.rename(columns = {i: colF5[i]})\n",
    "for i in range (0, len(colF6)):\n",
    "    OneHotTestF6 = OneHotTestF6.rename(columns = {i: colF6[i]})\n",
    "for i in range (0, len(colF7)):\n",
    "    OneHotTestF7 = OneHotTestF7.rename(columns = {i: colF7[i]})\n",
    "for i in range (0, len(colF8)):\n",
    "    OneHotTestF8 = OneHotTestF8.rename(columns = {i: colF8[i]})\n",
    "for i in range (0, len(colF9)):\n",
    "    OneHotTestF9 = OneHotTestF9.rename(columns = {i: colF9[i]})\n",
    "for i in range (0, len(colF13)):\n",
    "    OneHotTestF13 = OneHotTestF13.rename(columns = {i: colF13[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione newdf e newdftest con la sostituzione delle colonne generate con la codifica\n",
    "test_set_df = pd.concat([test_set_df, OneHotTestF1, OneHotTestF3, OneHotTestF5, OneHotTestF6, OneHotTestF7, OneHotTestF8, OneHotTestF9, OneHotTestF13], axis = 1)\n",
    " \n",
    "test_set_df = test_set_df.drop([\"F1\", \"F3\", \"F5\", \"F6\", \"F7\", \"F8\", \"F9\", \"F13\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[:] = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bestclf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copiare da ml la parte di confusion matrix"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
