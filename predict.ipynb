{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel notebook *predict* si caricano gli oggetti addestrati e si sfruttano per eseguire la routine di test completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indice\n",
    "\n",
    "### • Caricamento e pulizia dataset\n",
    "- Load\n",
    "- Feature Aggregation\n",
    "- Conversione feature categoriche\n",
    "- Imputazione valori NaN\n",
    "- Codifica OneHot\n",
    "\n",
    "### • Preprocessing\n",
    "- Scaling\n",
    "- Bilanciamento\n",
    "\n",
    "### • Classificazione\n",
    "- AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caricamento e pulizia dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load\n",
    "Nella cella sottostante vengono caricati gli oggetti *imputer*, *scaler*, *model* e la lista di codificatori OneHot.\n",
    "\n",
    "Quest'ultimo oggetto permette di risolvere il problema di un'eventuale assenza, nel test set, di alcuni valori presenti nel training set. Così facendo il numero di colonne risultanti sarà lo stesso, e le colonne relative a quei valori saranno comunque presenti, popolate da zeri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento degli oggetti addestrati\n",
    "imp = joblib.load(\"pickles/imputer.pkl\")\n",
    "scaler = joblib.load(\"pickles/scaler.pkl\")\n",
    "bestclf = joblib.load(\"pickles/model.pkl\")\n",
    "\n",
    "categorical = ['F1','F3','F5','F6','F7','F8','F9','F13']\n",
    "onehot_encoders = []\n",
    "for i in range(len(categorical)):\n",
    "    onehot_encoders.append(joblib.load(f\"pickles/ohenc-ohe{i}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lettura test set\n",
    "test = pd.read_csv(\"test_set.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Aggregation\n",
    "Eseguita nella stessa modalità usata in *train*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature aggregation: i Paesi sono stati raggruppati per continenti\n",
    "asia = [\"Philippines\", \"India\", \"South\", \"China\", \"Vietnam\", \"Japan\", \"Taiwan\", \"Iran\", \"Thailand\", \"Cambodia\", \"Laos\", \"Hong\"]\n",
    "europe = [\"Germany\", \"England\", \"Italy\", \"Poland\", \"Portugal\", \"Greece\", \"France\", \"Ireland\", \"Yugoslavia\", \"Hungary\", \"Scotland\", \"Netherlands\"]\n",
    "north_america = [\"USA\", \"Canada\"]\n",
    "central_america = [\"Mexico\", \"Puerto-Rico\", \"El-Salvador\", \"Cuba\", \"Jamaica\", \"Dominican-Republic\", \"Guatemala\", \"Haiti\", \"Nicaragua\", \"Caribbean\", \"Honduras\"]\n",
    "south_america = [\"Columbia\", \"Peru\", \"Ecuador\"]\n",
    "\n",
    "for i in range(len(test)):\n",
    "    if test.loc[i, \"F13\"] in asia:\n",
    "        test.F13.replace(test.loc[i, \"F13\"], \"asia\", inplace = True)\n",
    "    elif test.loc[i, \"F13\"] in europe:\n",
    "        test.F13.replace(test.loc[i, \"F13\"], \"europe\", inplace = True)\n",
    "    elif test.loc[i, \"F13\"] in north_america:\n",
    "        test.F13.replace(test.loc[i, \"F13\"], \"north_america\", inplace = True)\n",
    "    elif test.loc[i, \"F13\"] in central_america:\n",
    "        test.F13.replace(test.loc[i, \"F13\"], \"central_america\", inplace = True)\n",
    "    elif test.loc[i, \"F13\"] in south_america:\n",
    "        test.F13.replace(test.loc[i, \"F13\"], \"south_america\", inplace = True)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversione feature categoriche\n",
    "A ogni valore delle feature categoriche viene assegnato un valore numerico tramite la funzione *replace*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonne feature categoriche\n",
    "colF1 = ['K1', 'K2', 'K3', 'K4', 'K5', 'K6', 'K7', 'K8']\n",
    "colF3 = ['R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'R9', 'R10', 'R11', 'R12', 'R13', 'R14', 'R15', 'R16']\n",
    "colF5 = ['Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7']\n",
    "colF6 = ['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'P11', 'P12', 'P13', 'P14']\n",
    "colF7 = ['M1', 'M2', 'M3', 'M4', 'M5', 'M6']\n",
    "colF8 = ['caucasian', 'black', 'asian', 'american', 'other']\n",
    "colF9 = ['Male', 'Female']\n",
    "colF13 = ['asia', 'europe', 'north_america', 'central_america', 'south_america']\n",
    "\n",
    "# Codifica delle feature categoriche\n",
    "test.F1.replace(colF1, [0, 1, 2, 3, 4, 5, 6, 7], inplace = True)\n",
    "test.F3.replace(colF3, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], inplace = True)\n",
    "test.F5.replace(colF5, [0, 1, 2, 3, 4, 5, 6], inplace = True)\n",
    "test.F6.replace(colF6, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], inplace = True)\n",
    "test.F7.replace(colF7, [0, 1, 2, 3, 4, 5], inplace = True)\n",
    "test.F8.replace(colF8, [0, 1, 2, 3, 4], inplace = True)\n",
    "test.F9.replace(colF9, [0, 1], inplace = True)\n",
    "test.F13.replace(colF13, [0, 1, 2, 3, 4], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suddivisione tra insieme di punti (x) e target (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suddivisione tra feature e target\n",
    "x_test = test.iloc[:, :-1].values\n",
    "y_test = test.iloc[:, -1].values\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputazione valori NaN\n",
    "Di seguito viene chiamato il metodo *transform* sull'imputer caricato precedentemente per imputare i valori NaN presenti in x_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputazione dei valori NaN tramite SimpleImputer, con la strategia che sostituisce i valori mancanti con la moda dei valori della feature\n",
    "# Si utilizza l'oggetto caricato dal train\n",
    "x_test_notnan = imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dopo aver riempito le celle vuote, dal vettore risultante (x_test_notnan) viene creato il dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione DataFrame da insiemi di test dopo l'imputazione dei valori mancanti \n",
    "colF = ['F0','F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13']\n",
    "\n",
    "test_set_df = pd.DataFrame(x_test_notnan, columns = colF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codifica OneHot\n",
    "Per effettuare l'operazione di codifica viene dapprima creato un dataframe *onehot_df* vuoto, che verrà popolato iterativamente concatenandolo ai dataframe risultanti dalla codifica delle colonne categoriche (*transformed_df*).\n",
    "\n",
    "Si sottolinea che in questo notebook si usa solo il metodo *transform* in quanto si sta operando sul test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codifica OneHot: i valori delle feature categoriche vengono codificati secondo la tecnica OneHot\n",
    "# Si sfruttano gli oggetti Encoder addestrati in train e si invoca solo il metodo transform\n",
    "onehot_df = pd.DataFrame()\n",
    "columns = [colF1, colF3, colF5, colF6, colF7, colF8, colF9, colF13]\n",
    "categorical_df = pd.DataFrame()\n",
    "\n",
    "for i in categorical:\n",
    "    categorical_df = pd.concat([categorical_df, test_set_df[i]], axis = 1)\n",
    "\n",
    "j = 0\n",
    "\n",
    "# Creazione del dataframe con le colonne codificate\n",
    "for i in categorical:\n",
    "    # one hot encoding della colonna i\n",
    "    temp = pd.DataFrame(categorical_df[i].values)\n",
    "    transformed = onehot_encoders[j].transform(temp)\n",
    "\n",
    "    transformed_df = pd.DataFrame(transformed.toarray(), columns = columns[j], index = temp.index)\n",
    "\n",
    "    onehot_df = pd.concat([onehot_df, transformed_df], axis = 1)\n",
    "    j += 1\n",
    "\n",
    "onehot_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si crea un nuovo dataframe effettuando la concatenazione tra il dataframe di partenza e *onehot_df*, risultato della codifica delle feature categoriche; successivamente si esegue un drop delle vecchie colonne categoriche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unione con il DataFrame di partenza\n",
    "test_set_df_oh = pd.concat([test_set_df, onehot_df], axis = 1)\n",
    "test_set_df_oh = test_set_df_oh.drop(categorical, axis = 1)\n",
    "\n",
    "test_set_df_oh.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ricava x_test da quest'ultimo dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passaggio da DataFrame ad array\n",
    "x_test_notnan = test_set_df_oh.values\n",
    "print(x_test_notnan.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling\n",
    "Viene invocato il metodo *transform* sull'oggetto scaler caricato precedentemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling sfruttando l'oggetto caricato da train\n",
    "x_test_scaled = scaler.transform(x_test_notnan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "Viene finalmente effettuata la predizione sul test set, usando l'oggetto bestclf già caricato.\n",
    "\n",
    "I risultati sono graficati con una matrice di confusione e viene calcolata e stampata l'accuratezza in percentuale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predizione sul test set, con il classificatore caricato da train\n",
    "y_pred = bestclf.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione risultati sul test set (accuratezza)\n",
    "CM_ADA = confusion_matrix(y_test, y_pred)\n",
    "PercADA = (CM_ADA[0,0] + CM_ADA[1,1])/CM_ADA.sum()\n",
    "\n",
    "cmd = ConfusionMatrixDisplay(CM_ADA)\n",
    "cmd.plot(cmap = \"Oranges\")\n",
    "\n",
    "print(PercADA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b304063b9b5915891955c97f76410147208ba9c12388cc88c2e2efd4c5613ee8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
